---
title: "rethinking_lm"
format:
  html:
    toc: true
    df-print: paged
---

## Background information

-   Statistical inference: the process of drawing conclusions about a population based on a sample

-   Statistical model: a set of assumptions about the data-generating process

-   Likelihood: the probability of the data given the parameters of the model

-   Generative model: a model that specifies how data is generated (i.e., the likelihood of the data given the parameters)

-   Estimand: the unknown statistical quantity of interest

-   Estimator: a function of the data that estimates the estimand

-   Estimate: the value of the estimator given the data

-   Grid approximation: a method for estimating the posterior distribution by evaluating the prior and likelihood at a grid of parameter values

-   Posterior predictive check: a method for checking the fit of a model by comparing the distribution of data generated by the model to the distribution of the observed data

-   Linear models: a statistical model that assumes a linear relationship between the independent variable(s) and the dependent variable

## Setup environment

```{r}
#| output: false
library(coursekata)
library(tidyverse)
library(easystats)
library(rethinking)
library(rstanarm)
library(bayesplot)

theme_set(theme_bw())

set.rseed(666)
```

```{r}
sample.size <- 20 # number of samples in the dataset
n <- 1000         # number of samples used in Montecarlo simulations
```

## Load data

```{r}
data(Howell1) # from rethinking package
```

Demographic data from [Kalahari !Kung San people](https://en.wikipedia.org/wiki/%C7%83Kung_people) collected by [Nancy Howell](https://medium.com/extralife/the-long-ceiling-fddda72f4c0b)

-   `height`: Height in cm
-   `weight`: Weight in kg
-   `age`: Age in years
-   `male`: Gender indicator

## Explore data

```{r}
Howell1
```

```{r}
skimr::skim(Howell1)
```

```{r}
gf_point(weight ~ height, data = Howell1, alpha = 0.5, color = 4) %>%
  gf_smooth(linewidth = 1, color = 2, method = "loess") %>% 
  gf_labs(x = "Height (cm)", y = "Weight (kg)")
```

```{r}
d.pop <- Howell1 %>% filter(age >= 18) %>% select(-c(male, age))

d.pop
```

```{r}
skimr::skim(d.pop)
```

```{r}
gf_point(weight ~ height, data = d.pop, alpha = 0.5, color = 4) %>%
  gf_smooth(linewidth = 1, color = 2, method = "loess") %>% 
  gf_labs(x = "Height (cm)", y = "Weight (kg)")
```

```{r}
cor(weight ~ height, data = d.pop)
```

## Workflow

1.  State a clear question/goal
2.  Specify causal model (use a DAG to sketch your causal assumptions)
3.  Use causal model to build generative model
4.  Specify estimand (i.e., the unknown statistical quantity of interest)
5.  Use generative model to build estimator
6.  Test estimator (using data simulated with generative model)
7.  Fit/train model using observed data and estimate unknown quantity of interest
8.  Check model assumptions
9.  Check model predictions using training data (posterior predictive check)
10. Check model predictions using test data (replication and severe testing)
11. Profit

## State a clear question/goal

-   What is the relationship between height and weight in adults?

-   Describe the association between height and weight in adults

## Specify causal model (DAG)

### Deterministic function/relationship

```{mermaid}
graph LR;
    H --> W;
```

$$\text{W} = f(\text{H})$$

### Probabilistic function/relationship

```{mermaid}
graph LR;
    H --> W;
    U --> W;
```

$$\text{W} = f(\text{H, U})$$

## Use causal model to build generative model

### Deterministic function/relationship

Linear relationship:

$$
y_i = \beta_0 + \beta_1 \cdot x_i
$$

```{r}
sim_weight <- function(H, beta1) {
  W <- beta1 * H
  return(W)
}
```

```{r}
H <- rnorm(n = 350, mean = 155, sd = 8)
W <- sim_weight(H, beta1 = 0.4)
```

```{r}
d.sim <- tibble(height = H, weight = W)
```

```{r}
gf_point(weight ~ height, data = d.sim, alpha = 0.5, color = 3) %>%
  gf_smooth(linewidth = 1, color = 2, method = "loess") %>%
  gf_labs(x = "Height (cm)", y = "Weight (kg)")
```

### Probabilistic function/relationship

The likelihood of the general linear model:

$$
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i
$$

$$
\epsilon_i \stackrel{\text{i.i.d.}} \sim \text{Normal}(0, \sigma)
$$

or (equivalently):

$$
y_i \stackrel{\text{i.i.d.}} \sim \text{Normal}(\mu = (\beta_0 + \beta_1 \cdot x_i), \sigma)
$$

```{r}
sim_weight <- function(H, beta1, sigma) {
  U <- rnorm(length(H), mean = 0, sd = sigma)
  W <- beta1 * H + U
  return(W)
}
```

```{r}
H <- rnorm(n = 350, mean = 155, sd = 8)
W <- sim_weight(H, beta1 = 0.4, sigma = 4)
```

```{r}
d.sim <- tibble(height = H, weight = W)
```

```{r}
gf_point(weight ~ height, data = d.sim, alpha = 0.5, color = 3) %>%
  gf_smooth(linewidth = 1, color = 2, method = "loess") %>%
  gf_labs(x = "Height (cm)", y = "Weight (kg)")
```

## Specify estimand

The unknown statistical quantity of interest: the slope of the linear relationship between height and weight

## Grid approximation

### Use generative model to build estimator

Build Bayesian estimator using grid approximation

```{r}
# parameter variables
beta0 <- seq(-80, 80, length.out = 100)
beta1 <- seq(-3, 3, length.out = 100)
sigma <- seq(0.1, 20, length.out = 100)

# build sample space of parameter variables
ss <- expand_grid(beta0, beta1, sigma)

# simulate data
H <- rnorm(sample.size, mean = 155, sd = 8)
W <- sim_weight(H, beta1 = 0.4, sigma = 4)

# estimate using grid approximation
post <- ss %>%
  rowwise() %>%
  mutate(
    like = exp(sum(dnorm(W, mean = beta0 + beta1 * H, sigma, log = TRUE))),
    post.unnorm = like) %>%
  ungroup() %>%
  mutate(
    post = post.unnorm / sum(post.unnorm))

post
```

### Test estimator

$$
\beta_0 = 0 \quad \beta_1 = 0.4 \quad \sigma = 4
$$

```{r}
draws <- post %>%
  slice_sample(n = n, weight_by = post, replace = TRUE) %>%
  select(beta0, beta1, sigma) %>% 
  posterior::as_draws_df()
```

```{r}
mcmc_hist(draws, pars = "beta1", binwidth = 6/100)
```

```{r}
summary(draws)
```

### Fit/train model using observed data and estimate unknown quantity of interest

```{r}
# randomly sample data from population
d.smp <- d.pop %>% slice_sample(n = sample.size)

H <- d.smp$height # observed covariates
W <- d.smp$weight # observed outcomes

# estimate using grid approximation
post <- ss %>%
  rowwise() %>%
  mutate(
    like = exp(sum(dnorm(W, mean = beta0 + beta1 * H, sigma, log = TRUE))),
    post.unnorm = like) %>%
  ungroup() %>%
  mutate(
    post = post.unnorm / sum(post.unnorm))

post
```

```{r}
draws <- post %>%
  slice_sample(n = n, weight_by = post, replace = TRUE) %>%
  select(beta0, beta1, sigma) %>% 
  posterior::as_draws_df()
```

```{r}
mcmc_hist(draws, pars = "beta1", binwidth = 6/50)
```

```{r}
summary(draws)
```

### Check model assumptions and model predictions using training data

XXX

## Quadratic approximation

### Use generative model to build estimator

```{r}
m <- alist(
    W ~ dnorm(mu, sigma),
    mu <- beta0 + beta1 * H,
    beta0 ~ dnorm(0, 25),
    beta1 ~ dnorm(0, 1),
    sigma ~ dexp(0.3)
)
```

### Test estimator

$$
\beta_0 = 0 \quad \beta_1 = 0.4 \quad \sigma = 4
$$

```{r}
f <- quap(m, data = d.sim)
```

```{r}
summary(f)
```

### Fit/train model using observed data and estimate unknown quantity of interest

```{r}
f <- quap(m, data = d.smp)
```

```{r}
summary(f)
```

### Check model assumptions and model predictions using training data

XXX

## Bayesian linear model estimation (uniform priors)

### Use generative model to build estimator

XXX

### Test estimator

$$
\beta_0 = 0 \quad \beta_1 = 0.4 \quad \sigma = 4
$$

```{r}
#| output: false
bayes.est.unif <- stan_glm(weight ~ 1 + height, data = d.sim)
```

```{r}
summary(bayes.est.unif)
```

```{r}
estimate_slopes(bayes.est.unif, trend = "height")
```

### Fit/train model using observed data and estimate unknown quantity of interest

```{r}
#| output: false
bayes.est.unif <- stan_glm(weight ~ 1 + height, data = d.smp)
```

```{r}
summary(bayes.est.unif)
```

```{r}
estimate_slopes(bayes.est.unif, trend = "height")
```

### Check model assumptions and model predictions using training data

```{r}
performance::check_model(bayes.est.unif)
```

## Bayesian linear model estimation (uniform priors)

### Use generative model to build estimator

XXX

### Test estimator

$$
\beta_0 = 0 \quad \beta_1 = 0.4 \quad \sigma = 4
$$

```{r}
#| output: false
bayes.est.winf <- stan_glm(weight ~ 1 + height, data = d.sim,
                           prior = normal(0, 1),
                           prior_intercept = normal(0, 25),
                           prior_aux = exponential(0.3))
```

```{r}
summary(bayes.est.winf)
```

```{r}
estimate_slopes(bayes.est.winf, trend = "height")
```

### Fit/train model using observed data and estimate unknown quantity of interest

```{r}
#| output: false
bayes.est.winf <- stan_glm(weight ~ 1 + height, data = d.smp,
                           prior = normal(0, 1),
                           prior_intercept = normal(0, 25),
                           prior_aux = exponential(0.3))
```

```{r}
summary(bayes.est.winf)
```

```{r}
estimate_slopes(bayes.est.winf, trend = "height")
```

### Check model assumptions and model predictions using training data

```{r}
performance::check_model(bayes.est.winf)
```

## Maximum likelihood linear model estimation

### Use generative model to build estimator

XXX

### Test estimator

$$
\beta_0 = 0 \quad \beta_1 = 0.4 \quad \sigma = 4
$$

```{r}
ml.est <- lm(weight ~ 1 + height, data = d.sim)
```

```{r}
summary(ml.est)
```

```{r}
estimate_slopes(ml.est, trend = "height")
```

### Fit/train model using observed data and estimate unknown quantity of interest

```{r}
ml.est <- lm(weight ~ 1 + height, data = d.smp)
```

```{r}
summary(ml.est)
```

```{r}
estimate_slopes(ml.est, trend = "height")
```

### Check model assumptions and model predictions using training data

```{r}
performance::check_model(ml.est)
```

## Compare models

```{r}
modelsummary::modelsummary(list(mle = ml.est, bayes.unif = bayes.est.unif, bayes.winf = bayes.est.winf), metrics = "all")
```

## Test model using new data (replication)

XXX

## Profit but keep testing (severely)

XXX

## Error probabilities

-   Scientific/statistical hypotheses must be **testable**/falsifiable/refutable and these tests must be **severe** (i.e., have a high probability of detecting a false hypothesis/proving a hypothesis wrong/rejecting a hypothesis if it is wrong)

-   To be testable scientific/statistical hypotheses must make precise and risky predictions

-   Scientific/statistical hypotheses cannot be confirmed to be true, only corroborated by surviving several and diverse severe tests

-   Standard NHST methodology is not severe testing

    -   Hypothesis of interest: $H_0: \beta_1 = 0 \quad H_1: \beta_1 \neq 0$

    -   Inference procedure: reject $H_0$/accept $H_1$ if $p < 0.05$

-   Types of probability:

    -   **Frequentist probability**: the long-run relative frequency of an event

    -   **Bayesian probability**: the degree of rational belief in a proposition/strength of inductive argument

    -   **Error probability**: the probability of making a mistake in a decision

-   Types of statistical inference:

    -   **Frequentist inference**: the process of drawing conclusions about a population based on a sample

    -   **Bayesian inference**: the process of updating beliefs about a population based on prior knowledge and observed data

    -   **Error probability inference**: the process of controlling the probability of making a mistake in a decision

When testing under the null (where each individual test has a 5% chance of a false positive), the probability of finding at least one significant result across multiple independent tests is given by:

$$P(\text{at least one false positive}) = 1 - (1 - \alpha)^n$$

with $\alpha = 0.05$ and $n$ being the number of tests. Using this formula, here’s how it works out from 1 to 20 hypotheses:

```{r}
tibble(n = 1:20) %>% mutate(prob = 1 - (1 - 0.05)^n) %>% knitr::kable()
```

```{r}
tibble(n = 1:200) %>%
  mutate(prob = 1 - (1 - 0.05)^n) %>%
  gf_line(prob ~ n) %>%
  gf_labs(x = "Number of tests", y = "Probability of at least one false positive")
```
